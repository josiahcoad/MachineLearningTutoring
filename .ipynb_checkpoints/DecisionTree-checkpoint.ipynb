{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Made by Josiah Coad, CoadBros-Tutoring\n",
    "- Date: Jun 12, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "#### When do you use this model? \n",
    "- You have labeled data (a DT is a supervised machine learning model)\n",
    "- You want a highly interpretable model (easy and intuitive to see how the computer is making decisions)\n",
    "- Your problem is a classification problem (although there are also regression applications)\n",
    "- You have a large data set (thousands of samples per label)\n",
    "    - (A decision tree is a non-linear algorithm which means that it can capture complex relationships and non-linearities in your data but has a tendency to overfit your data)\n",
    "- You need a model set up quickly\n",
    "\n",
    "#### Difference between random forest and decision trees\n",
    "- A random forest is a collection of decision trees where each tree chooses a subset of features from the dataset to build from. \n",
    "- The subset of features for each tree are chosen at random.\n",
    "- As a result, each tree will be like \"experts\" in the features they were allowed to use.\n",
    "- They all pitch in to the final decision like \"votes\" towards the final decision.\n",
    "- Example: As the president, you want many people who are around you who all have different backgrounds to give you input on the decisions you should make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Decision Tree\n",
    "#### You will need to install the following python packages using Anaconda or pip:\n",
    "- scipy, sklearn, numpy, statistics\n",
    "\n",
    "#### The tree graph file is a .dot file. I suggest either of the following ways to view the .dot:\n",
    "1. Using xdot\n",
    "    - brew install xdot\n",
    "2. Using graphviz:\n",
    "    - brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import sklearn.tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal-length  sepal-width  petal-length  petal-width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, an example with the iris dataset using pandas\n",
    "# Let's load the iris dataset...\n",
    "iris_filename = 'iris.csv'\n",
    "feature_columns = [\"sepal-length\", \"sepal-width\", \"petal-length\", \"petal-width\"]\n",
    "label_column = [\"class\"]\n",
    "# read in the dataframe\n",
    "iris_df = pd.read_csv(iris_filename)\n",
    "# seperate the features and labels\n",
    "iris_data = iris_df[feature_columns]\n",
    "iris_labels = iris_df[label_column]\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclassifier = sklearn.tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extra\n",
    "- look into additional parameters to pass to DecisionTreeClassifier [link here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- start with adding __min_impurity_split=.1__ or __max_depth=5__ as a parameter\n",
    "- see how your tree changes and how your cross_validation changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where the magic happens...\n",
    "iris_dtree = dtclassifier.fit(iris_data, iris_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at the output of fitting our model!\n",
    "sklearn.tree.export_graphviz(iris_dtree,\n",
    "    feature_names=[\"Sepal Length\", \"Sepal Width\", \"Petal Length\",\"Petal Width\"],\n",
    "    class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"],\n",
    "    out_file='iris_tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run the above code, type in your terminal (based on the install method you chose): \n",
    "1. xdot iris_tree.dot\n",
    "2. dot -Tpng iris_tree.dot -o iris_tree.png && open iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In case you weren't able to do those steps, here is the output:\n",
    "![Decision tree for Iris](iris_tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95999999999999996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now see how well your iris model performed!\n",
    "statistics.mean(sklearn.model_selection.cross_val_score(iris_dtree, iris_data, iris_labels, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gini index for a split dataset\n",
    "# gini_index goes from 0 to 1\n",
    "# A gini_index of 0 is the best meaning that each bucket\n",
    "# has homogeneous data. AKA each bucket has data\n",
    "# from a single class and no bucket has data from 2 or more classes.\n",
    "def gini_index(buckets, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(bucket) for bucket in buckets]))\n",
    "    # sum weighted Gini index for each bucket\n",
    "    gini = 0.0\n",
    "    for bucket in buckets:\n",
    "        size = float(len(bucket))\n",
    "        # avoid divide by zero for buckets with no data in them\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the bucket based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in bucket].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset split into two buckets based\n",
    "# on a split point (feature, feature value).\n",
    "# Return the left/right children buckets.\n",
    "def test_split(splitfeature_index, splitvalue, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[splitfeature_index] < splitvalue:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Select the best split point (feature, feature value) for a dataset\n",
    "# Iterate through each feature and each corresponding feature value,\n",
    "# and \"test\" what would happen if we split on that split point.\n",
    "# save the split point where the \"test results\" AKA gini value\n",
    "# is closest to 0.\n",
    "def get_split(dataset):\n",
    "    # Note here that the class label is assumed to be the last column of the dataset \n",
    "    class_values = list(set(dataset[class_column]))\n",
    "    best_gini, bestfeature_index, bestsample_index, best_groups = 99999, 99999, 99999, None\n",
    "    features = range(len(dataset[0]) - 1)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < best_gini:\n",
    "                bestfeature_index, bestsample_index, best_gini, best_groups = index, row[index], gini, groups\n",
    "    return {'index':bestfeature_index, 'value':bestsample_index, 'groups':best_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal/leaf node\n",
    "def to_terminal(bucket):\n",
    "    outcomes = [row[-1] for row in bucket]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "# Call recursively \n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree by splitting the data many times\n",
    "def build_tree(training_data, max_depth, min_size):\n",
    "    root = get_split(training_data)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using our own .fit function that we made!\n",
    "sonar_dtree = build_tree(iris_df, max_depth=10, min_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These package are just to display the json in an interactive way\n",
    "import uuid\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the tree as an interactive json object\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "\n",
    "    def _ipython_display_(self):\n",
    "        display_html('<div id=\"{}\" style=\"height: 600px; width:100%;\"></div>'.format(self.uuid), raw=True)\n",
    "        display_javascript(\"\"\"\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "        document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });\n",
    "        \"\"\" % (self.uuid, self.json_str), raw=True)\n",
    "\n",
    "RenderJSON(sonar_dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case the above didn't work...\n",
    "print(json.dumps(sonar_dtree, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap what is important\n",
    "- The tree is made from the root, down to its leaves/terminals by calling get_split recursively on each child node.\n",
    "- The computer is trying at each step/node to get the lowest gini possible.\n",
    "- The lowest gini is achieved when the left/right buckets are as homogenous (data from only one class) as possible\n",
    "- At each node, the computer iterates through each feature and each value corresponding to that feature and tries to split the data given that split point (feature, feature value) and sees how good of a gini index it can get.\n",
    "- If left to itself, the computer would grow the tree until every node had a gini index of 0... but that would probably be overfitting the data so we set a max_depth.\n",
    "- The code from scratch was very inefficient. What is the time complexity of this algorithm? How could you improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next...\n",
    "- A decision tree has probably overfit your data.\n",
    "- Try a random forest (which is a ensemble method) next to try to mitigate the overfitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
