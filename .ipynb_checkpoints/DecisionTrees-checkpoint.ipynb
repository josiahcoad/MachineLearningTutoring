{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests from Scratch\n",
    "### Tutorial from https://machinelearningmastery.com/implement-random-forest-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, you are looking at a new restaurant and wondering if you should go to it. You have a \"data set\" \\[price, location, food quality, type of food, wasGoodChoice\\] of all the past restaurants you went to... how are you going to make your decision? Go to the restaurant or not??\n",
    "1. Get an opinion from a friend who has also been to all the same restaurant with you and cares about everything exactly the same as you. They will give you thier input and thats how you decide. Whats wrong with this?\n",
    "    - This is just too biased on the decision of one friend, what about getting more friends to chip in?\n",
    "    - If that person is have a bad day they may decide widely different then if they were having a good day. Your one friend has \"high variability\".\n",
    "    - This is analogous to a single decision tree.\n",
    "2. So now you have 10 friends that all care about the same things as you, but they each only went to a few of the restaurants that you have. You ask them all to pitch in thier opinion.\n",
    "    - They all have slighly different backgrounds so they may give you a different answer but they all care about the exact same things, so they will most likely have vary similar opinions.\n",
    "    - You mitigated the \"high variability\" of only one friend, but they're decision making process is going to have \"high correlation\" with eachother.\n",
    "    - This is analogous to \"bagging\".\n",
    "3. So you take 10 friends who all care about different features of the restaurant and each only went to a few of the restaurants that you have. i.e. Joe cares about price and location and could care less about food quality and type of food. You ask them all to pitch in thier opinion.\n",
    "    - This time you've found a happy middle between variability (one friend) and correlation (friends with same taste)\n",
    "    - Each friend is going to pick slighly different based on different criteria and you get to take all their input to make a final decision.\n",
    "    - This happy medium is called random forest algorithm :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the decision tree algorithm...\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    \n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
