{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests from Scratch\n",
    "- Step-by-step tutorial [here](https://machinelearningmastery.com/implement-random-forest-scratch-python/)\n",
    "- Video walkthrough [here](https://www.youtube.com/watch?v=QHOazyP-YlM)\n",
    "- Podcast [here](https://podtail.com/en/podcast/data-skeptic/mini-decision-tree-learning/) and [here](https://www.acast.com/dataskeptic/-mini-random-forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, you are looking at a new restaurant and wondering if you should go to it. You have a \"data set\" \\[price, location, food quality, type of food, wasGoodChoice\\] of all the past restaurants you went to... how are you going to make your decision? Go to the restaurant or not??\n",
    "1. Get an opinion from a friend who has also been to all the same restaurant with you and cares about everything exactly the same as you. They will give you thier input and thats how you decide. Whats wrong with this?\n",
    "    - This is just too biased on the decision of one friend, what about getting more friends to chip in?\n",
    "    - If that person is have a bad day they may decide widely different then if they were having a good day. Your one friend has \"high variability\".\n",
    "    - This is analogous to a single decision tree.\n",
    "2. So now you have 10 friends that all care about the same things as you, but they each only went to a few of the restaurants that you have. You ask them all to pitch in thier opinion.\n",
    "    - They all have slighly different backgrounds so they may give you a different answer but they all care about the exact same things, so they will most likely have vary similar opinions.\n",
    "    - You mitigated the \"high variability\" of only one friend, but they're decision making process is going to have \"high correlation\" with eachother.\n",
    "    - This is analogous to \"bagging\".\n",
    "3. So you take 10 friends who all care about different features of the restaurant and each only went to a few of the restaurants that you have. i.e. Joe cares about price and location and could care less about food quality and type of food. You ask them all to pitch in thier opinion.\n",
    "    - This time you've found a happy middle between variability (one friend) and correlation (friends with same taste)\n",
    "    - Each friend is going to pick slighly different based on different criteria and you get to take all their input to make a final decision.\n",
    "    - This happy medium is called random forest algorithm :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next four code boxes are copied nearly directly from the tutorial linked at the beginning. Run them all through before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Algorithm on Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the decision tree algorithm...\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    # features_subset = random.sample(range(1, len(dataset[0])-1), n_features)\n",
    "    features_subset = list()\n",
    "    while len(features_subset) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features_subset.append(index)\n",
    "    # go through each feature in the subset of features\n",
    "    for feature_index in features_subset:\n",
    "        # go through each data sample\n",
    "        for row in dataset:\n",
    "            # split the data set into two groups based on a canidate split threshold\n",
    "            groups = test_split(index, row[feature_index], dataset)\n",
    "            # access the success of that split to correctly\n",
    "            # place the data in its correct categories\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = feature_index, row[feature_index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        # weight the group score by its relative size\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "sonar_filename = 'sonar.all-data.csv'\n",
    "sonar_dataset = load_csv(sonar_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [56.09756097560976, 63.41463414634146, 60.97560975609756, 58.536585365853654, 73.17073170731707]\n",
      "Mean Accuracy: 62.439%\n",
      "Trees: 5\n",
      "Scores: [70.73170731707317, 58.536585365853654, 85.36585365853658, 75.60975609756098, 63.41463414634146]\n",
      "Mean Accuracy: 70.732%\n",
      "Trees: 10\n",
      "Scores: [82.92682926829268, 75.60975609756098, 97.5609756097561, 80.48780487804879, 68.29268292682927]\n",
      "Mean Accuracy: 80.976%\n"
     ]
    }
   ],
   "source": [
    "# Test the random forest algorithm\n",
    "seed(2)\n",
    "# convert string attributes to integers\n",
    "for i in range(0, len(sonar_dataset[0])-1):\n",
    "    str_column_to_float(sonar_dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(sonar_dataset, len(sonar_dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 10\n",
    "min_size = 1\n",
    "sample_size = 1.0\n",
    "n_features = int(sqrt(len(sonar_dataset[0])-1))\n",
    "for n_trees in [1, 5, 10]:\n",
    "    scores = evaluate_algorithm(sonar_dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now lets go through it. How is a tree constructed?\n",
    "- Each node uses a \"gini cost function\" to determine which threshold to pick for that node.\n",
    "- The goal is to find ideal threashold that puts every (as many as possible) data point from class A in the right-hand bucket and every other point in the class B in the left-hand bucket.\n",
    "- Gini value of 0 is a \"pure bucket\", meaning in the LH (or RH) bucket, all the points are from class A or all from class B.\n",
    "- Gini value of .5 is a 50/50 bucket, meaning in the LH (or RH) bucket, half the points are from class A and half class B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Decision Tree\n",
    "#### You will need to install the following:\n",
    "- pip3 install scipy\n",
    "- brew install xdot\n",
    "    - or you can:\n",
    "        - brew install graphviz\n",
    "        - dot -Tpng DocName.dot -o DocName.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first a dummy example\n",
    "iris = load_iris()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "tree.export_graphviz(clf,\n",
    "    feature_names=[\"Sepal Length\", \"Sepal Width\", \"Petal Length\",\"Petal Width\"],\n",
    "    class_names=[\"Setosa\", \"Versicolour\", \"Virginica\"],\n",
    "    out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for the next part\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with the sonar data set from above\n",
    "sonar_df = pd.read_csv(sonar_filename, header=None)\n",
    "sonar_data, sonar_labels = np.split(sonar_df, [60], axis=1)\n",
    "# here we are converting the labels {\"R\", \"M\"} to {0, 1}\n",
    "# note to self: find better way to make series from dataframe\n",
    "sonar_labels = pd.factorize(sonar_labels.T.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more to come ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
